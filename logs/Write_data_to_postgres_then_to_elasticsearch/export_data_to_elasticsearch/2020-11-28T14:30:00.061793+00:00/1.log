[2020-11-28 20:00:25,494] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: Write_data_to_postgres_then_to_elasticsearch.export_data_to_elasticsearch 2020-11-28T14:30:00.061793+00:00 [queued]>
[2020-11-28 20:00:25,525] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: Write_data_to_postgres_then_to_elasticsearch.export_data_to_elasticsearch 2020-11-28T14:30:00.061793+00:00 [queued]>
[2020-11-28 20:00:25,525] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2020-11-28 20:00:25,525] {taskinstance.py:881} INFO - Starting attempt 1 of 1
[2020-11-28 20:00:25,526] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2020-11-28 20:00:25,544] {taskinstance.py:901} INFO - Executing <Task(BashOperator): export_data_to_elasticsearch> on 2020-11-28T14:30:00.061793+00:00
[2020-11-28 20:00:25,548] {standard_task_runner.py:54} INFO - Started process 47004 to run task
[2020-11-28 20:00:25,578] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'Write_data_to_postgres_then_to_elasticsearch', 'export_data_to_elasticsearch', '2020-11-28T14:30:00.061793+00:00', '--job_id', '494', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/csv_pg_elasticsearch.py', '--cfg_path', '/var/folders/ff/pqhf5klx0msdsn3m1cnq4cxr0000gn/T/tmpr7k4qla7']
[2020-11-28 20:00:25,581] {standard_task_runner.py:78} INFO - Job 494: Subtask export_data_to_elasticsearch
[2020-11-28 20:00:25,632] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: Write_data_to_postgres_then_to_elasticsearch.export_data_to_elasticsearch 2020-11-28T14:30:00.061793+00:00 [running]> 47.43.168.192.in-addr.arpa
[2020-11-28 20:00:25,663] {bash_operator.py:113} INFO - Tmp dir root location: 
 /var/folders/ff/pqhf5klx0msdsn3m1cnq4cxr0000gn/T
[2020-11-28 20:00:25,664] {bash_operator.py:136} INFO - Temporary script location: /var/folders/ff/pqhf5klx0msdsn3m1cnq4cxr0000gn/T/airflowtmpp7ulbqae/export_data_to_elasticsearch7i63cui0
[2020-11-28 20:00:25,664] {bash_operator.py:146} INFO - Running command: /Users/yudi/ELK/logstash-7.10.0/bin/logstash -f logstash-simple.conf
[2020-11-28 20:00:25,669] {bash_operator.py:153} INFO - Output:
[2020-11-28 20:00:25,698] {bash_operator.py:157} INFO - Using JAVA_HOME defined java: /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home
[2020-11-28 20:00:25,699] {bash_operator.py:157} INFO - WARNING, using JAVA_HOME while Logstash distribution comes with a bundled JDK
[2020-11-28 20:00:46,248] {bash_operator.py:157} INFO - Sending Logstash logs to /Users/yudi/ELK/logstash-7.10.0/logs which is now configured via log4j2.properties
[2020-11-28 20:00:46,793] {bash_operator.py:157} INFO - [2020-11-28T20:00:46,789][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.10.0", "jruby.version"=>"jruby 9.2.13.0 (2.5.7) 2020-08-03 9a89c94bcc OpenJDK 64-Bit Server VM 25.262-b10 on 1.8.0_262-b10 +indy +jit [darwin-x86_64]"}
[2020-11-28 20:00:47,120] {bash_operator.py:157} INFO - [2020-11-28T20:00:47,119][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2020-11-28 20:00:47,134] {bash_operator.py:157} INFO - [2020-11-28T20:00:47,134][FATAL][logstash.runner          ] Logstash could not be started because there is already another instance using the configured data directory.  If you wish to run multiple instances, you must change the "path.data" setting.
[2020-11-28 20:00:47,139] {bash_operator.py:157} INFO - [2020-11-28T20:00:47,139][ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit
[2020-11-28 20:00:47,214] {bash_operator.py:161} INFO - Command exited with return code 1
[2020-11-28 20:00:47,240] {taskinstance.py:1150} ERROR - Bash command failed
Traceback (most recent call last):
  File "/Users/yudi/opt/anaconda3/envs/airflow/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/yudi/opt/anaconda3/envs/airflow/lib/python3.6/site-packages/airflow/operators/bash_operator.py", line 165, in execute
    raise AirflowException("Bash command failed")
airflow.exceptions.AirflowException: Bash command failed
[2020-11-28 20:00:47,245] {taskinstance.py:1194} INFO - Marking task as FAILED. dag_id=Write_data_to_postgres_then_to_elasticsearch, task_id=export_data_to_elasticsearch, execution_date=20201128T143000, start_date=20201128T143025, end_date=20201128T143047
[2020-11-28 20:00:50,566] {local_task_job.py:102} INFO - Task exited with return code 1
